{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:25:43.545627Z",
     "start_time": "2020-04-24T11:25:28.558527Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "import pandas as pd\n",
    "from plotnine import *\n",
    "from jupyterplot import ProgressPlot\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as backend\n",
    "from threading import Thread\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#check os\n",
    "try:\n",
    "    sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (\n",
    "        sys.version_info.major,\n",
    "        sys.version_info.minor,\n",
    "        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])\n",
    "except IndexError:\n",
    "    pass\n",
    "\n",
    "import carla\n",
    "print('IMPORT DONE')\n",
    "\n",
    "#recommend to use conda install in the library of Tensorflow and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Environment [Action/Reward here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:25:46.535882Z",
     "start_time": "2020-04-24T11:25:46.489008Z"
    }
   },
   "outputs": [],
   "source": [
    "class CarEnv:\n",
    "\n",
    "    def __init__(self):\n",
    "        # connect to carla\n",
    "        self.client = carla.Client('localhost', 2000)\n",
    "        self.client.set_timeout(5.0)\n",
    "        self.world = self.client.get_world()\n",
    "        \n",
    "        # change the view of camera to the place we want to train\n",
    "        spectator = self.world.get_spectator()\n",
    "        spectator.set_transform(carla.Transform(carla.Location(-142,110,2)))\n",
    "        \n",
    "        # destroy all vehices and sensors to prepare the environment\n",
    "        for x in list(self.world.get_actors()):\n",
    "            if 'vehicle' in x.type_id or 'sensor' in x.type_id:\n",
    "                x.destroy()\n",
    "        \n",
    "        # spawn a vehicle\n",
    "        blueprint_library = self.world.get_blueprint_library()\n",
    "        self.Isetta = blueprint_library.filter('Isetta')[0]\n",
    "        transform = carla.Transform(carla.Location(-127,121,3),carla.Rotation(0,-90,0))\n",
    "        self.vehicle = self.world.spawn_actor(self.Isetta, transform)\n",
    "        \n",
    "        # set vehicle parameter\n",
    "        self.front_left_wheel  = carla.WheelPhysicsControl(max_steer_angle=44.0, radius=25.39)\n",
    "        self.front_right_wheel = carla.WheelPhysicsControl(max_steer_angle=48.0, radius=25.39)\n",
    "        self.rear_left_wheel   = carla.WheelPhysicsControl(max_steer_angle=0.0,  radius=25.39)\n",
    "        self.rear_right_wheel  = carla.WheelPhysicsControl(max_steer_angle=0.0,  radius=25.39)\n",
    "        self.wheels = [self.front_left_wheel, self.front_right_wheel, self.rear_left_wheel, self.rear_right_wheel]\n",
    "        physics_control = self.vehicle.get_physics_control()\n",
    "        physics_control.wheels = self.wheels\n",
    "        physics_control.mass = 490.8\n",
    "        physics_control.gear_switch_time = 0.0\n",
    "        self.vehicle.apply_physics_control(physics_control)\n",
    "        \n",
    "        # set the lidar parameter and attach it to the vehicle\n",
    "        self.lidar_sensor = self.world.get_blueprint_library().find('sensor.lidar.ray_cast')\n",
    "        self.lidar_sensor.set_attribute('points_per_second', '300000')\n",
    "        self.lidar_sensor.set_attribute('channels', '16')\n",
    "        self.lidar_sensor.set_attribute('range', '100')\n",
    "        self.lidar_sensor.set_attribute('upper_fov', '10')\n",
    "        self.lidar_sensor.set_attribute('lower_fov', '-10')\n",
    "        self.lidar_sensor.set_attribute('rotation_frequency', '20')\n",
    "        transform = carla.Transform(carla.Location(x=0, z=1.875))\n",
    "        self.sensor = self.world.spawn_actor(self.lidar_sensor, transform, attach_to=self.vehicle)\n",
    "        \n",
    "        # set the colsensor parameter and attach it to the vehicle\n",
    "        transform2 = carla.Transform(carla.Location(x=1.1975, z=0.7)) #หน้ารถ HA:MO ยาว 1.1975 เมตร\n",
    "        colsensor = self.world.get_blueprint_library().find('sensor.other.collision')\n",
    "        self.colsensor = self.world.spawn_actor(colsensor, transform2, attach_to=self.vehicle)\n",
    "        \n",
    "        # get data from the sensors\n",
    "        self.sensor.listen(lambda data: self.process_lidar(data))\n",
    "        self.colsensor.listen(lambda event: self.collision_data(event))\n",
    "        \n",
    "        # sleep to get things start\n",
    "        time.sleep(4)\n",
    "        \n",
    "        # define collision history as a list\n",
    "        self.collision_hist = []\n",
    "        \n",
    "    def Black_screen(self):\n",
    "        # if you want to train in no-rendering mode of CARLA \n",
    "        settings = self.world.get_settings()\n",
    "        settings.no_rendering_mode = True\n",
    "        self.world.apply_settings(settings)\n",
    "    \n",
    "    def get_fps():\n",
    "        # get the fps at the moment of CARLA \n",
    "        world_snapshot = self.world.get_snapshot()\n",
    "        fps = 1/world_snapshot.timestamp.delta_seconds\n",
    "        return fps\n",
    "          \n",
    "    def reset(self): \n",
    "        # reset environment\n",
    "        # after the crash, the car may bounce off the ground. Teleport it back to the floor and stop the car.\n",
    "        self.vehicle.set_transform(carla.Transform(carla.Location(-127, 125, 0.5),carla.Rotation(0, -90, 0)))\n",
    "        self.vehicle.set_angular_velocity(carla.Vector3D(x=0.0, y=0.0, z=0.0))\n",
    "        self.vehicle.set_velocity(carla.Vector3D(x=0.0, y=0.0, z=0.0))\n",
    "        \n",
    "        # turn the steered wheel back\n",
    "        self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, steer=0.0, brake=0.0))\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # teleport the vehicle to the start point and set the initial velocity \n",
    "        self.vehicle.set_transform(carla.Transform(carla.Location(-127,125,0.5),carla.Rotation(0,-90,0)))\n",
    "        self.vehicle.set_velocity(carla.Vector3D(x=0.0, y=-10, z=0.0))\n",
    "        time.sleep(0.2) \n",
    "        \n",
    "        self.collision_hist = []\n",
    "        self.episode_start = time.time()\n",
    "        \n",
    "        xx = self.distance_to_obstacle\n",
    "        vv = -self.vehicle.get_velocity().y\n",
    "        aa = 0\n",
    "        sign = 0\n",
    "        state_=np.array([xx, vv, aa, sign])\n",
    "       \n",
    "        return state_\n",
    "\n",
    "    def collision_data(self, event):\n",
    "        self.collision_hist.append(event)\n",
    "\n",
    "    def process_lidar(self, raw):\n",
    "        # get distance to the obstacle(wall)\n",
    "        points = np.frombuffer(raw.raw_data, dtype=np.dtype('f4'))\n",
    "        points = np.reshape(points, (int(points.shape[0] / 3), 3))*np.array([1,-1,-1])\n",
    "        points_filter = lidar_line(points, 90, 1)\n",
    "        if len(points_filter) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            # minus length of the front car hood\n",
    "            self.distance_to_obstacle = min(points_filter[:,1])-1.1898 \n",
    "\n",
    "    def step(self, action):\n",
    "        # action and reward\n",
    "        global sleepy  \n",
    "        \n",
    "        if action == 0:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=1.0, brake=0.0, steer = 0.0))\n",
    "            time.sleep(sleepy) # @FPS = 10, should use sleepy = 0.3\n",
    "        elif action == 1:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=0.0, steer = 0.0))\n",
    "            time.sleep(sleepy) # @FPS = 10, should use sleepy = 0.3\n",
    "        elif action == 2:\n",
    "            self.vehicle.apply_control(carla.VehicleControl(throttle=0.0, brake=1.0, steer = 0.0))\n",
    "            time.sleep(sleepy) # @FPS = 10, should use sleepy = 0.3    \n",
    "  \n",
    "        xx = self.distance_to_obstacle\n",
    "        vv = -self.vehicle.get_velocity().y\n",
    "        aa = -self.vehicle.get_acceleration().y \n",
    "        if xx >= 8:\n",
    "            sign = 0 # normal signal\n",
    "        else:\n",
    "            sign =1 # brake signal\n",
    " \n",
    "        state_=np.array([xx,vv,aa,sign])\n",
    " \n",
    "        kmh = abs(int(3.6 * vv))\n",
    "    \n",
    "        # set velocity and collision end terminal\n",
    "        if kmh == 0 and len(self.collision_hist) == 0 :\n",
    "            done = True\n",
    "            if -1<= xx <8:\n",
    "                reward = 1/36*((-xx**2) + 4*xx+32)\n",
    "            elif  8<= xx :\n",
    "                reward = 0\n",
    "        elif len(self.collision_hist) != 0 :\n",
    "            done = True\n",
    "            reward = -1\n",
    "        else:\n",
    "            done = False\n",
    "            reward = 0  \n",
    "        \n",
    "        #set time-up end terminal\n",
    "        if self.episode_start + SECONDS_PER_EPISODE < time.time():\n",
    "            done = True\n",
    "            \n",
    "        return state_, reward, done, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:25:50.447647Z",
     "start_time": "2020-04-24T11:25:50.400774Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to open CARLA Simulator. Please change the path to yours before use.\n",
    "def open_carla(require):\n",
    "    try:\n",
    "        if require == 'fast':\n",
    "            os.popen('D:\\\\Desktop\\\\CARLA_0.9.8\\\\WindowsNoEditor\\\\CarlaUE4.exe -benchmark  -fps=10 -quality-level=Low -ResX=360 -ResY=240')\n",
    "        elif type(require) == int:\n",
    "            os.popen('D:\\\\Desktop\\\\carla_0.9.8_D\\\\WindowsNoEditor\\\\CarlaUE4.exe -benchmark  -fps=' + str(require)+' -quality-level=Low -ResX=360 -ResY=240')\n",
    "        else:\n",
    "            os.popen('D:\\\\Desktop\\\\carla_0.9.8_D\\\\WindowsNoEditor\\\\CarlaUE4.exe -benchmark  -fps=90 -quality-level=Low -ResX=360 -ResY=240')\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "    print('opening Carla')\n",
    "\n",
    "# function to close CARLA Simulator.\n",
    "def close_carla():\n",
    "    try:\n",
    "        os.system('TASKKILL /F /IM CarlaUE4.exe')\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# function to check whether the CARLA Simulator is running.\n",
    "def carla_is_running():\n",
    "    import psutil    \n",
    "    if \"CarlaUE4.exe\" in (p.name() for p in psutil.process_iter()):\n",
    "        return True\n",
    "\n",
    "# function to destroy all the vehicles and sensors\n",
    "def xxx():\n",
    "    for x in list(env.world.get_actors()):\n",
    "        if 'vehicle' in x.type_id or 'sensor' in x.type_id:\n",
    "            x.destroy()\n",
    "\n",
    "# function to screen the 3D lidar point cloud to a width straight line in the quadrant degree direction from the car \n",
    "# unit is metre\n",
    "def lidar_line(points, degree, width):\n",
    "    angle = degree*(2*np.pi)/360\n",
    "    points_l = points\n",
    "    points_l = points_l[np.logical_and(points_l[:,2] > -1, points_l[:,2] < 1000)] #z\n",
    "    points_l = points_l[np.logical_and(np.tan(angle)*points_l[:,0]+width*np.sqrt(1+np.tan(angle)**2)>=points_l[:,1], \n",
    "                                       np.tan(angle)*points_l[:,0]-width*np.sqrt(1+np.tan(angle)**2)<=points_l[:,1])] #y\n",
    "    if 180>degree >0:\n",
    "        points_l = points_l[np.logical_and(points_l[:,1]>0, points_l[:,1]<1000)] #y>0\n",
    "    if 180<degree<360:\n",
    "        points_l = points_l[np.logical_and(points_l[:,1]<0, points_l[:,1] > -1000)] #x\n",
    "    if degree == 0 or degree == 360:\n",
    "        points_l = points_l[np.logical_and(points_l[:,0]>0,points_l[:,0] <1000 )] #x\n",
    "    if degree == 180:\n",
    "        points_l = points_l[np.logical_and(points_l[:,0] >-1000 , points_l[:,0]<0 )]\n",
    "    return  points_l\n",
    "\n",
    "# function to save the data and model every 5000 episodes, please change the path before use.\n",
    "def save_every_n_episode(num_of_episode):\n",
    "        # data prep.\n",
    "        n=cum=avg=0   \n",
    "        avg_loss=[]\n",
    "        for i in Loss[1:]:\n",
    "            n+=1\n",
    "            cum+=i\n",
    "            avg=cum/n\n",
    "            avg_loss.append(avg)\n",
    "\n",
    "        df=pd.DataFrame({'Episode':ep,'Reward':ep_rewards,\n",
    "                         'avg_reward':avg_reward,'Step':Step,'Loss':Loss[1:],\n",
    "                         'avg_loss':avg_loss,\n",
    "                         'Explore':Explore,'PCT_Explore':np.array(Explore)/np.array(Step)*100,\n",
    "                         'Epsilon':Epsilon, 'Dist_stop':Dist_stop,\n",
    "                         'Max_accel':Max_accel, 'Avg_accel':Avg_accel})\n",
    "        if LOAD == True:\n",
    "            df=pd.concat([df_load,df],ignore_index=True)   \n",
    "\n",
    "        ###save data\n",
    "        name='full_deep_brake_cont_reward_limit_action@' + str(num_of_episode) ##INSERT FILE NAME\n",
    "        n = datetime.datetime.now()\n",
    "        n = n.strftime('_%m%d%y_%H%M')\n",
    "\n",
    "        ##real\n",
    "        file_path=\"DATA\\\\\"\n",
    "        df.to_csv(file_path+'{}.csv'.format(name))\n",
    "        agent.save_model(file_path+name)\n",
    "\n",
    "        ##backup\n",
    "        name=name+n\n",
    "        file_path_backup=\"DATA\\\\backup\\\\\"\n",
    "        df.to_csv(file_path_backup+'{}.csv'.format(name))\n",
    "        agent.save_model(file_path_backup+name)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:25:53.023598Z",
     "start_time": "2020-04-24T11:25:52.992348Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        self.target_update_counter = 0\n",
    "        #self.graph = tf.get_default_graph()\n",
    "\n",
    "        self.terminate = False\n",
    "        self.last_logged_episode = 0\n",
    "        self.training_initialized = False\n",
    "    \n",
    "    def get_weight(self):\n",
    "        \n",
    "        w = self.model.get_weights()        \n",
    "        return w\n",
    "    \n",
    "    def predict(self,state):\n",
    "        \n",
    "        predict = self.model.predict(state.reshape((1, self.state_size)))\n",
    "        return predict\n",
    "    \n",
    "    def save_model(self,name):\n",
    "        model_json = self.model.to_json()\n",
    "        with open(\"{}.json\".format(name), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        self.model.save_weights(\"{}.h5\".format(name))\n",
    "        print(\"Saved model to disk\")     \n",
    "        \n",
    "\n",
    "    def create_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(4, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(4, input_dim=4, activation='relu')) \n",
    "        model.add(Dense(self.action_size, activation='sigmoid'))            # output nodes = #action\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=0.01))\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        # transition = (current_state, action, reward, new_state, done)\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self):\n",
    "        global Loss\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            self.terminate=True\n",
    "            Loss.append(0)\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "        \n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        \n",
    "        #with self.graph.as_default():\n",
    "        current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE)\n",
    "\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
    "        \n",
    "        #with self.graph.as_default():\n",
    "        future_qs_list = self.target_model.predict(new_current_states, PREDICTION_BATCH_SIZE)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "            \n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "\n",
    "        history=self.model.fit(np.array(X), np.array(y), batch_size=TRAINING_BATCH_SIZE, verbose=0, shuffle=False)\n",
    "        history\n",
    "        Loss.append(history.history['loss'][0])\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(state.reshape((1, self.state_size)))[0]\n",
    "        \n",
    "    def train_in_loop(self):\n",
    "        X = np.random.uniform(size=(1, self.state_size)).astype(np.float32)\n",
    "        y = np.random.uniform(size=(1, self.action_size)).astype(np.float32)\n",
    "        \n",
    "        self.model.fit(X,y, verbose=False, batch_size=1)\n",
    "\n",
    "        self.training_initialized = True\n",
    "        print('Start Train')\n",
    "        while True:\n",
    "            if self.terminate:\n",
    "                return\n",
    "            self.train()\n",
    "            time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:25:53.545562Z",
     "start_time": "2020-04-24T11:25:53.514312Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent_load_model:\n",
    "    \n",
    "    def __init__(self,state_size,action_size,model):\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.model = self.loaded_model(model)\n",
    "        self.target_model = self.loaded_model(model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        self.target_update_counter = 0\n",
    "        #self.graph = tf.get_default_graph()\n",
    "\n",
    "        self.terminate = False\n",
    "        self.last_logged_episode = 0\n",
    "        self.training_initialized = False\n",
    "        \n",
    "    def loaded_model(self,model):\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=Adam(lr=0.01))\n",
    "        return model\n",
    "    \n",
    "    def get_weight(self):\n",
    "        \n",
    "        w = self.model.get_weights()        \n",
    "        return w\n",
    "    \n",
    "    def predict(self,state):\n",
    "        \n",
    "        predict = self.model.predict(state.reshape((1, self.state_size)))\n",
    "        return predict\n",
    "    \n",
    "    def save_model(self,name):\n",
    "        model_json = self.model.to_json()\n",
    "        with open(\"{}.json\".format(name), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        self.model.save_weights(\"{}.h5\".format(name))\n",
    "        print(\"Saved model to disk\")     \n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        # transition = (current_state, action, reward, new_state, done)\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self):\n",
    "        global Loss\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            self.terminate=True\n",
    "            Loss.append(0)\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "        \n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        \n",
    "        #with self.graph.as_default():\n",
    "        current_qs_list = self.model.predict(current_states, PREDICTION_BATCH_SIZE)\n",
    "\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
    "        \n",
    "        #with self.graph.as_default():\n",
    "        future_qs_list = self.target_model.predict(new_current_states, PREDICTION_BATCH_SIZE)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "            \n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "\n",
    "        history=self.model.fit(np.array(X), np.array(y), batch_size=TRAINING_BATCH_SIZE, verbose=0, shuffle=False)\n",
    "        history\n",
    "        Loss.append(history.history['loss'][0])\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(state.reshape((1, self.state_size)))[0]\n",
    "        \n",
    "    def train_in_loop(self):\n",
    "        X = np.random.uniform(size=(1, self.state_size)).astype(np.float32)\n",
    "        y = np.random.uniform(size=(1, self.action_size)).astype(np.float32)\n",
    "        \n",
    "        self.model.fit(X,y, verbose=False, batch_size=1)\n",
    "\n",
    "        self.training_initialized = True\n",
    "        print('Start Train')\n",
    "        while True:\n",
    "            if self.terminate:\n",
    "                return\n",
    "            self.train()\n",
    "            time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:25:54.868645Z",
     "start_time": "2020-04-24T11:25:54.853023Z"
    }
   },
   "outputs": [],
   "source": [
    "SECONDS_PER_EPISODE = 12\n",
    "REPLAY_MEMORY_SIZE = 5_000\n",
    "MIN_REPLAY_MEMORY_SIZE = 32\n",
    "MINIBATCH_SIZE = 32\n",
    "PREDICTION_BATCH_SIZE = 1\n",
    "TRAINING_BATCH_SIZE = MINIBATCH_SIZE // 4\n",
    "UPDATE_TARGET_EVERY = 5\n",
    "\n",
    "MEMORY_FRACTION = 0.4\n",
    "\n",
    "EPISODES = 50_000\n",
    "\n",
    "DISCOUNT = 0.99\n",
    "epsilon = 1\n",
    "EPSILON_DECAY = 0.9999539 # epsilon = 0.01 at 50,000th episode\n",
    "MIN_EPSILON = 0.01\n",
    "\n",
    "AGGREGATE_STATS_EVERY = 10\n",
    "state_size=4\n",
    "action_size=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:25:55.402527Z",
     "start_time": "2020-04-24T11:25:55.386903Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate formula of the epsilon decay\n",
    "d_____ = np.exp((np.log(0.1))/50000)\n",
    "d_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW model or LOAD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:25:57.853177Z",
     "start_time": "2020-04-24T11:25:57.837555Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    json_file = open('D:\\\\Desktop\\\\Full report\\\\Brake\\\\Deep brake with maximaization bias\\\\{}.json'.format(model_name), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"D:\\\\Desktop\\\\Full report\\\\Brake\\\\Deep brake with maximaization bias\\\\{}.h5\".format(model_name))\n",
    "    \n",
    "    print(\"Loaded model from disk\")\n",
    "    loaded_model.summary()\n",
    "    \n",
    "    file_path=\"D:\\\\Desktop\\\\Full report\\\\Brake\\\\Deep brake with maximaization bias\\\\\"\n",
    "    df=pd.read_csv(file_path+'{}.csv'.format(model_name))\n",
    "    episode = df.Episode.tail(1).values[0]+1\n",
    "    epsilon = df.Epsilon.tail(1).values[0]\n",
    "    print('Episode : {} , Epsilon : {} '.format(episode,epsilon))\n",
    "    \n",
    "    return df,episode,epsilon,loaded_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:25:59.775220Z",
     "start_time": "2020-04-24T11:25:59.759594Z"
    }
   },
   "outputs": [],
   "source": [
    "# in the first time of use, please change \"LOAD = False'\n",
    "LOAD = False\n",
    "model_name='full_deep_brake_cont_reward_@30000'\n",
    "if LOAD == True :\n",
    "    df_load,load_episode,load_epsilon,loaded_model = load_model(model_name)\n",
    "    value=['Episode', 'Reward', 'avg_reward', 'Step', 'Loss', 'avg_loss', 'Explore',\n",
    "           'PCT_Explore', 'Epsilon', 'Dist_stop', 'Max_accel', 'Avg_accel']\n",
    "    df_load=df_load[value]\n",
    "    df_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:28:45.216645Z",
     "start_time": "2020-04-24T11:28:10.162425Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    if carla_is_running():\n",
    "        print('Carla is running')\n",
    "    else:\n",
    "        close_carla()\n",
    "        open_carla('fast')\n",
    "        # Wait for CARLA Simulator to open. It will take a long time in the first time of openning the CARLA.\n",
    "        time.sleep(17)\n",
    "    \n",
    "    ep_rewards = []\n",
    "    ep=[]\n",
    "    avg=0\n",
    "    avg_reward=[]\n",
    "    Step=[]\n",
    "    Loss=[]\n",
    "    Explore=[]\n",
    "    Max_accel=[]\n",
    "    Avg_accel=[]\n",
    "    Epsilon=[]\n",
    "    Dist_stop=[]\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    sleepy=0.3\n",
    "\n",
    "    pp = ProgressPlot(x_label=\"Episode\",line_names=['Average_reward'])\n",
    "    \n",
    "    if LOAD == True:\n",
    "    # In case Train from loaded_model\n",
    "        agent=DQNAgent_load_model(state_size,action_size,loaded_model)\n",
    "        epsilon=load_epsilon\n",
    "        nn=0\n",
    "        for i in df_load.Reward:\n",
    "            avg=((avg*(nn)+i)/(nn+1))\n",
    "            pp.update(float(avg))\n",
    "            nn+=1\n",
    "        avg=sum(df_load.Reward)/df_load.shape[0]\n",
    "    else :\n",
    "        \n",
    "    # Create agent and environment\n",
    "        agent = DQNAgent(state_size,action_size)\n",
    "        load_episode=0\n",
    "    \n",
    "    env = CarEnv()\n",
    "    #env.Black_screen()\n",
    "    \n",
    "    agent.train_in_loop()\n",
    "    agent.get_qs(np.ones((1, state_size)))\n",
    "    \n",
    "    # if you want to Re_epsilon\n",
    "    ###epsilon = 0.7\n",
    "    \n",
    "    # Iterate over episodes\n",
    "    with tqdm(total=EPISODES-load_episode) as pbar:\n",
    "        \n",
    "        for episode in range(EPISODES-load_episode):            \n",
    "\n",
    "            env.collision_hist = []\n",
    "            episode_reward = 0\n",
    "            Accel=[]\n",
    "            loss=0\n",
    "            step = 1\n",
    "            explore=0\n",
    "\n",
    "            # Reset environment and get initial state\n",
    "            current_state = env.reset()\n",
    "            \n",
    "            # Reset flag and start iterating until episode ends\n",
    "            done = False\n",
    "            episode_start = time.time()\n",
    "\n",
    "            # Play for given number of seconds only\n",
    "            while True:\n",
    "\n",
    "                # This part stays mostly the same, the change is to query a model for Q values\n",
    "                rand=np.random.random()\n",
    "                if rand > epsilon:\n",
    "                    # Get action from Q table\n",
    "                    action = np.argmax(agent.get_qs(current_state))\n",
    "                    new_state, reward, done, _ = env.step(action)\n",
    "                    \n",
    "                else:\n",
    "                    # Get random action\n",
    "                    action = np.random.randint(0, action_size)\n",
    "                    new_state, reward, done, _ = env.step(action)\n",
    "                    explore+=1\n",
    "             \n",
    "                # Every step we update replay memory\n",
    "                episode_reward += reward\n",
    "                agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "                current_state = new_state\n",
    "                Accel.append(current_state[2])\n",
    "                step += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            agent.train()\n",
    "                    \n",
    "            # Decay epsilon\n",
    "            if epsilon > MIN_EPSILON:\n",
    "                epsilon *= EPSILON_DECAY\n",
    "                epsilon = max(MIN_EPSILON, epsilon)\n",
    "                \n",
    "            print('Episode :{}, Step :{}, Epsilon :{} ,Reward :{}, Explore_rate :{}, loss :{} '\\\n",
    "                  .format(episode+load_episode,step,epsilon,episode_reward,explore/step,Loss[episode]))\n",
    "            \n",
    "            ep_rewards.append(episode_reward)\n",
    "            ep.append(episode+load_episode)\n",
    "            Step.append(step)\n",
    "            Explore.append(explore)\n",
    "            Max_accel.append(max(Accel))\n",
    "            Avg_accel.append(sum(Accel)/len(Accel))\n",
    "            Epsilon.append(epsilon)\n",
    "            avg=((avg*(episode+load_episode)+episode_reward)/(episode+load_episode+1))\n",
    "            avg_reward.append(avg)\n",
    "            pp.update(float(avg))  \n",
    "            Dist_stop.append(current_state[0])\n",
    "            \n",
    "            num_of_episode_ = episode+load_episode+1\n",
    "            if num_of_episode_ % 5_000 == 0 and num_of_episode_ != 0:\n",
    "                save_every_n_episode(num_of_episode_)\n",
    "            \n",
    "            pbar.update(1)\n",
    "    close_carla()\n",
    "    pp.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:30:22.059813Z",
     "start_time": "2020-04-24T11:30:22.054816Z"
    }
   },
   "outputs": [],
   "source": [
    "## CUMULATIVE AVERAGE ##\n",
    "\n",
    "n=cum=avg=0   \n",
    "avg_loss=[]\n",
    "for i in Loss[1:]:\n",
    "    n+=1\n",
    "    cum+=i\n",
    "    avg=cum/n\n",
    "    avg_loss.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:30:23.704800Z",
     "start_time": "2020-04-24T11:30:23.564886Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Episode':ep,'Reward':ep_rewards,\n",
    "                 'avg_reward':avg_reward,'Step':Step,'Loss':Loss[1:],\n",
    "                 'avg_loss':avg_loss,\n",
    "                 'Explore':Explore,'PCT_Explore':np.array(Explore)/np.array(Step)*100,\n",
    "                 'Epsilon':Epsilon, 'Dist_stop':Dist_stop,\n",
    "                 'Max_accel':Max_accel,'Avg_accel':Avg_accel})\n",
    "if LOAD == True:\n",
    "    df=pd.concat([df_load,df],ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:30:41.865523Z",
     "start_time": "2020-04-24T11:30:41.621673Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:30:46.220536Z",
     "start_time": "2020-04-24T11:30:45.581929Z"
    }
   },
   "outputs": [],
   "source": [
    "n=0\n",
    "c=0\n",
    "xx=[]\n",
    "for i in df['Reward']:\n",
    "    n+=i\n",
    "    c+=1\n",
    "    if c%100==0:\n",
    "        xx.append(n/100)\n",
    "        n=0\n",
    "        \n",
    "df__=pd.DataFrame({'reward':xx,'avg_every_100ep':[i for i in range(len(xx))]})\n",
    "\n",
    "ggplot(df__, aes(x='avg_every_100ep',y='reward'))+ \\\n",
    "    geom_line(size=0.6,alpha=0.5) +\\\n",
    "    stat_smooth(colour='blue', span=0.2,linetype='dashed')+theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T04:28:57.348334Z",
     "start_time": "2020-04-10T04:28:56.885378Z"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df,aes(x='Reward'))+\\\n",
    "geom_histogram(binwidth=0.5,fill='#48b6a3')+\\\n",
    "xlim(-1.5,1.5)+theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T04:29:01.766547Z",
     "start_time": "2020-04-10T04:28:57.352336Z"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df, aes(x='Episode',y='Reward'))+ \\\n",
    "  geom_point(color=\"#F18F01\", size=0.2)+\\\n",
    "    geom_smooth(method=\"lm\", se=False, size=0.5, linetype=\"dashed\")+\\\n",
    "        ylim(-1.25,1.25)+theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T04:29:05.559665Z",
     "start_time": "2020-04-10T04:29:01.766547Z"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df, aes(x='Episode',y='Step'))+ \\\n",
    "    geom_line(size=0.05) +\\\n",
    "    stat_smooth(colour='blue', span=0.2)+\\\n",
    "        ylim(0,30)+theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T04:29:09.101684Z",
     "start_time": "2020-04-10T04:29:05.559665Z"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df, aes(x='Episode',y='avg_reward'))+ \\\n",
    "    geom_line(size=0.7,color='seagreen') +\\\n",
    "    stat_smooth(colour='blue', span=0.3,size=0.4)+theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T04:29:13.531128Z",
     "start_time": "2020-04-10T04:29:09.101684Z"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df, aes(x='Episode',y='Step'))+ \\\n",
    "  geom_point(color=\"#F18F01\", size=0.2)+\\\n",
    "    geom_smooth(method=\"lm\", se=False, size=0.5, linetype=\"dashed\")+\\\n",
    "        ylim(0,30)+theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T04:29:14.400189Z",
     "start_time": "2020-04-10T04:29:13.534128Z"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df, aes(x='Episode',y='Loss'))+ \\\n",
    "  geom_line(size=0.01,color='mediumblue') +\\\n",
    "    geom_smooth(method=\"lm\",color='firebrick', se=False, size=0.5, linetype=\"dashed\")+\\\n",
    "ylim(0,0.05)+theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T04:29:18.726939Z",
     "start_time": "2020-04-10T04:29:14.400189Z"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df, aes(x='Episode',y='avg_loss'))+ \\\n",
    "  geom_point(color='midnightblue',size=0.05) +\\\n",
    "    geom_smooth(method=\"lm\", se=False, size=0.5, linetype=\"dashed\")+theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T04:29:26.013918Z",
     "start_time": "2020-04-10T04:29:18.726939Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ggplot(df, aes(x='Episode',y='PCT_Explore'))+ \\\n",
    "  geom_line(color='midnightblue',size=0.05) +\\\n",
    "    geom_point(color=\"orchid\", size=0.1)+\\\n",
    "    geom_smooth( size=1, linetype=\"dashed\")+theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T05:30:54.800456Z",
     "start_time": "2020-04-10T05:30:54.002954Z"
    }
   },
   "outputs": [],
   "source": [
    "name='full_deep_brake_cont_reward' ##INSERT FILE NAME\n",
    "n = datetime.datetime.now()\n",
    "n = n.strftime('_%m%d%y_%H%M')\n",
    "\n",
    "##real\n",
    "file_path=\"D:\\\\Desktop\\\\Full report\\\\Brake\\\\Deep brake with maximaization bias\\\\\"\n",
    "df.to_csv(file_path+'{}.csv'.format(name))\n",
    "agent.save_model(file_path+name)\n",
    "\n",
    "##backup\n",
    "name=name+n\n",
    "file_path_backup=\"D:\\\\Desktop\\\\Full report\\\\Brake\\\\Deep brake with maximaization bias\\\\\"\n",
    "df.to_csv(file_path_backup+'{}.csv'.format(name))\n",
    "agent.save_model(file_path_backup+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:35:15.802630Z",
     "start_time": "2020-04-24T11:35:15.432858Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name='full_deep_brake_cont_reward_limit_action@50000'\n",
    "\n",
    "json_file = open('C:\\\\Users\\\\Lee\\\\CARLA_0.9.8\\\\WindowsNoEditor\\\\PythonAPI\\\\examples\\\\DATA\\\\t\\\\{}.json'.format(model_name), 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"C:\\\\Users\\\\Lee\\\\CARLA_0.9.8\\\\WindowsNoEditor\\\\PythonAPI\\\\examples\\\\DATA\\\\t\\\\{}.h5\".format(model_name))\n",
    "print(\"Loaded model from disk\")\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:36:49.139789Z",
     "start_time": "2020-04-24T11:35:20.227298Z"
    }
   },
   "outputs": [],
   "source": [
    "if carla_is_running():\n",
    "    print('carla is running')\n",
    "else:\n",
    "    close_carla()\n",
    "    open_carla('fast')\n",
    "    time.sleep(17)\n",
    "\n",
    "test_ep = 10\n",
    "test_step = []\n",
    "test_reward = []\n",
    "dto = []\n",
    "dto_err = []\n",
    "sleepy = 0.3\n",
    "\n",
    "env = CarEnv()\n",
    "\n",
    "state_size = loaded_model.input.shape[1]\n",
    "action_size = loaded_model.output.shape[1]\n",
    "print('State size = {} // Action size = {} '.format(state_size,action_size))\n",
    "    \n",
    "with tqdm(total=test_ep) as pbar:\n",
    "    for episode in range(test_ep):\n",
    "        \n",
    "        state=env.reset()\n",
    "        \n",
    "        done = False\n",
    "        step = 0\n",
    "        rewards = 0\n",
    "        steer_ = 0\n",
    "            \n",
    "        while True:\n",
    "            step += 1\n",
    "            state = np.array([[i for i in state]])\n",
    "            action = np.argmax(loaded_model.predict(state))\n",
    "            time.sleep(0.01)\n",
    "            state,reward,done,_ = env.step(action)\n",
    "            rewards += reward\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        dto.append(state[0])   \n",
    "        dto_err.append(2-state[0]) # we set 2-metre-from-the-wall length as a goal of the car to stop\n",
    "        test_reward.append(rewards)\n",
    "        test_step.append(step)\n",
    "        pbar.update(1)\n",
    "xxx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:36:51.430378Z",
     "start_time": "2020-04-24T11:36:51.417387Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test=pd.DataFrame({'Episode':[i for i in range(len(test_step))],\\\n",
    "                      'Step':test_step,'Reward':test_reward,'distance to obstacle':dto, 'distance error':dto_err})\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:36:53.859882Z",
     "start_time": "2020-04-24T11:36:53.834897Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:36:58.247179Z",
     "start_time": "2020-04-24T11:36:55.959588Z"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(df_test, aes(x='Episode',y='Reward'))+ \\\n",
    "  geom_point(color=\"#F18F01\", size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T11:37:28.622063Z",
     "start_time": "2020-04-24T11:37:27.445789Z"
    }
   },
   "outputs": [],
   "source": [
    "close_carla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "KWY Contents",
   "title_sidebar": "KWY Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
